# main.py - MLflowê°€ í†µí•©ëœ AI ì„œë²„
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse
from pydantic import BaseModel
import torch
import clip
import cv2
import numpy as np
from PIL import Image
from pathlib import Path
from ultralytics import YOLO
import shutil
from typing import List, Dict, Any
import uvicorn
import base64
import io
import time
import uuid
from datetime import datetime

# MLflow ê´€ë ¨ imports
import mlflow
import mlflow.pytorch
import mlflow.sklearn
from mlflow.tracking import MlflowClient

# FastAPI ì•± ìƒì„±
app = FastAPI(title="CCTV AI ë¶„ì„ ì„œë²„ (MLflow í†µí•©)", version="3.0.0")

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# MLflow ì„¤ì •
mlflow.set_tracking_uri("http://localhost:5000")  # MLflow ì„œë²„ ì£¼ì†Œ
mlflow.set_experiment("CCTV_Person_Detection")

# ë°ì´í„° ì €ì¥ í´ë”
ROOT_DIR = Path('./cctv_data')
VIDEO_DIR = ROOT_DIR / 'videos'
CROP_DIR = ROOT_DIR / 'crops'
DATA_DIR = ROOT_DIR / 'data'
MLFLOW_DIR = ROOT_DIR / 'mlruns'

# í´ë” ìƒì„±
for folder in [ROOT_DIR, VIDEO_DIR, CROP_DIR, DATA_DIR, MLFLOW_DIR]:
    folder.mkdir(parents=True, exist_ok=True)

# ì „ì—­ ë³€ìˆ˜
device = 'cuda' if torch.cuda.is_available() else 'cpu'
yolo_model = None
clip_model = None
clip_preprocess = None

# í˜„ì¬ ì‹¤í—˜ ì„¤ì • (í•˜ì´í¼íŒŒë¼ë¯¸í„°)
current_config = {
    "yolo_confidence_threshold": 0.5,
    "yolo_model_size": "yolov8n",  # nano, small, medium, large
    "clip_model_type": "ViT-B/32",
    "max_frames_per_video": 30,
    "min_person_size": 50,
    "search_similarity_threshold": 0.1
}

# ìš”ì²­/ì‘ë‹µ ëª¨ë¸
class SearchRequest(BaseModel):
    query: str
    k: int = 5

class SearchResult(BaseModel):
    image_path: str
    caption: str
    score: float
    image_base64: str = ""

class ExperimentConfig(BaseModel):
    yolo_confidence_threshold: float = 0.5
    yolo_model_size: str = "yolov8n"
    max_frames_per_video: int = 30
    min_person_size: int = 50

# ë¶„ì„ëœ ë°ì´í„° ì €ì¥
analyzed_data = {
    "embeddings": None,
    "image_paths": [],
    "captions": [],
    "images": [],
    "experiment_id": None,
    "run_id": None
}

# ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì €ì¥
performance_metrics = {
    "total_videos_processed": 0,
    "total_persons_detected": 0,
    "average_detection_confidence": 0.0,
    "processing_times": [],
    "search_accuracies": []
}

@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ì‹œ AI ëª¨ë¸ ë¡œë“œ ë° MLflow ì´ˆê¸°í™”"""
    global yolo_model, clip_model, clip_preprocess
    
    try:
        print("ğŸš€ MLflow í†µí•© AI ì„œë²„ ì‹œì‘")
        print("=" * 60)
        
        # MLflow ì‹¤í—˜ ì‹œì‘
        with mlflow.start_run(run_name=f"Server_Startup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"):
            print("ğŸ“Š MLflow ì‹¤í—˜ ì‹œì‘...")
            
            # ì‹œìŠ¤í…œ ì •ë³´ ë¡œê¹…
            mlflow.log_param("device", device)
            mlflow.log_param("pytorch_version", torch.__version__)
            mlflow.log_param("cuda_available", torch.cuda.is_available())
            
            # í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¡œê¹…
            for key, value in current_config.items():
                mlflow.log_param(key, value)
            
            print("ğŸ¤– AI ëª¨ë¸ ë¡œë”© ì¤‘...")
            start_time = time.time()
            
            # YOLO ëª¨ë¸ ë¡œë“œ
            print(f"ğŸ“¦ YOLO ëª¨ë¸ ë¡œë”©: {current_config['yolo_model_size']}")
            yolo_model = YOLO(f"{current_config['yolo_model_size']}.pt")
            yolo_load_time = time.time() - start_time
            
            # CLIP ëª¨ë¸ ë¡œë“œ
            print(f"ğŸ§  CLIP ëª¨ë¸ ë¡œë”©: {current_config['clip_model_type']}")
            clip_start = time.time()
            clip_model, clip_preprocess = clip.load(current_config['clip_model_type'], device=device)
            clip_load_time = time.time() - clip_start
            
            # ë¡œë”© ì‹œê°„ ë¡œê¹…
            mlflow.log_metric("yolo_load_time_seconds", yolo_load_time)
            mlflow.log_metric("clip_load_time_seconds", clip_load_time)
            mlflow.log_metric("total_load_time_seconds", time.time() - start_time)
            
            # ëª¨ë¸ ì •ë³´ ë¡œê¹…
            mlflow.log_param("yolo_model_loaded", True)
            mlflow.log_param("clip_model_loaded", True)
            mlflow.log_param("total_parameters", "estimated_millions")  # ì‹¤ì œ ê³„ì‚° ê°€ëŠ¥
            
            print("âœ… ëª¨ë“  AI ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
            print(f"   - YOLO ë¡œë”© ì‹œê°„: {yolo_load_time:.2f}ì´ˆ")
            print(f"   - CLIP ë¡œë”© ì‹œê°„: {clip_load_time:.2f}ì´ˆ")
            print(f"   - ì´ ë¡œë”© ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ")
            
            # ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ ì €ì¥ ì¤€ë¹„ (ì„ íƒì‚¬í•­)
            # mlflow.pytorch.log_model(clip_model, "clip_model")
            
        print("ğŸ“Š MLflow ì´ˆê¸°í™” ì™„ë£Œ")
        print("ğŸŒ MLflow UI: http://localhost:5000")
        print("=" * 60)
        
    except Exception as e:
        print(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        with mlflow.start_run():
            mlflow.log_param("initialization_error", str(e))

@app.get("/")
async def ë©”ì¸í˜ì´ì§€():
    """ì„œë²„ ë©”ì¸ í˜ì´ì§€"""
    return {
        "ë©”ì‹œì§€": "ğŸš€ MLflow í†µí•© AI ë¶„ì„ ì„œë²„",
        "ë²„ì „": "3.0.0",
        "MLflow_UI": "http://localhost:5000",
        "ëª¨ë¸_ìƒíƒœ": {
            "YOLO": "ë¡œë”©ë¨" if yolo_model else "ë¡œë”©ì‹¤íŒ¨",
            "CLIP": "ë¡œë”©ë¨" if clip_model else "ë¡œë”©ì‹¤íŒ¨"
        },
        "í˜„ì¬_ì„¤ì •": current_config,
        "ì„±ëŠ¥_í†µê³„": performance_metrics
    }

@app.get("/health")
async def ìƒíƒœí™•ì¸():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    return {
        "ìƒíƒœ": "ì •ìƒ",
        "AI_ì¤€ë¹„": all([yolo_model, clip_model]),
        "MLflow_ì—°ê²°": True,
        "ì‹¤í—˜_ì •ë³´": {
            "í˜„ì¬_ì‹¤í—˜": mlflow.get_experiment_by_name("CCTV_Person_Detection"),
            "í™œì„±_ëŸ°": analyzed_data.get("run_id")
        }
    }

def detect_persons_in_video_with_mlflow(video_path: Path, run_id: str = None):
    """MLflow ì¶”ì ì´ í¬í•¨ëœ ë¹„ë””ì˜¤ ì‚¬ëŒ íƒì§€"""
    
    if not yolo_model:
        raise Exception("YOLO ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    # MLflow ì‹¤í—˜ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì‹¤í–‰
    with mlflow.start_run(run_id=run_id, nested=True) as run:
        
        # ë¹„ë””ì˜¤ ì •ë³´ ë¡œê¹…
        mlflow.log_param("video_filename", video_path.name)
        mlflow.log_param("video_size_mb", video_path.stat().st_size / 1024 / 1024)
        
        print(f"ğŸ¬ ë¹„ë””ì˜¤ ë¶„ì„ ì‹œì‘: {video_path.name}")
        processing_start_time = time.time()
        
        cap = cv2.VideoCapture(str(video_path))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        duration = total_frames / fps if fps > 0 else 0
        
        # ë¹„ë””ì˜¤ ë©”íƒ€ë°ì´í„° ë¡œê¹…
        mlflow.log_param("total_frames", total_frames)
        mlflow.log_param("fps", fps)
        mlflow.log_param("duration_seconds", duration)
        
        detected_persons = []
        frame_count = 0
        confidences = []
        
        # í”„ë ˆì„ ê°„ê²© ê³„ì‚°
        max_frames = current_config["max_frames_per_video"]
        frame_interval = max(1, total_frames // max_frames)
        mlflow.log_param("frame_interval", frame_interval)
        mlflow.log_param("frames_to_analyze", min(max_frames, total_frames))
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
                
            frame_count += 1
            
            if frame_count % frame_interval != 0:
                continue
                
            try:
                # YOLO ì¶”ë¡  ì‹œê°„ ì¸¡ì •
                inference_start = time.time()
                results = yolo_model(frame, classes=[0], verbose=False)
                inference_time = time.time() - inference_start
                
                for r in results:
                    boxes = r.boxes
                    if boxes is not None:
                        for box in boxes:
                            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                            confidence = box.conf[0].cpu().numpy()
                            
                            # ì‹ ë¢°ë„ ì„ê³„ê°’ ì ìš©
                            if confidence > current_config["yolo_confidence_threshold"]:
                                x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)
                                
                                # í¬ê¸° ì„ê³„ê°’ ì ìš©
                                if (x2 - x1) > current_config["min_person_size"] and (y2 - y1) > current_config["min_person_size"]:
                                    person_img = frame[y1:y2, x1:x2]
                                    person_pil = Image.fromarray(cv2.cvtColor(person_img, cv2.COLOR_BGR2RGB))
                                    
                                    filename = f"{video_path.stem}_frame{frame_count}_person{len(detected_persons)}.jpg"
                                    crop_path = CROP_DIR / filename
                                    person_pil.save(crop_path)
                                    
                                    detected_persons.append({
                                        "íŒŒì¼ê²½ë¡œ": str(crop_path),
                                        "í”„ë ˆì„ë²ˆí˜¸": frame_count,
                                        "ì‹ ë¢°ë„": float(confidence),
                                        "ë°•ìŠ¤ì¢Œí‘œ": [x1, y1, x2, y2],
                                        "ì´ë¯¸ì§€": person_pil,
                                        "ì¶”ë¡ ì‹œê°„": inference_time
                                    })
                                    
                                    confidences.append(float(confidence))
            
            except Exception as e:
                mlflow.log_param(f"frame_{frame_count}_error", str(e))
                continue
        
        cap.release()
        processing_time = time.time() - processing_start_time
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚° ë° ë¡œê¹…
        avg_confidence = np.mean(confidences) if confidences else 0.0
        detection_rate = len(detected_persons) / (total_frames / frame_interval) if total_frames > 0 else 0
        
        mlflow.log_metric("persons_detected", len(detected_persons))
        mlflow.log_metric("average_confidence", avg_confidence)
        mlflow.log_metric("detection_rate", detection_rate)
        mlflow.log_metric("processing_time_seconds", processing_time)
        mlflow.log_metric("fps_processing", (total_frames / frame_interval) / processing_time if processing_time > 0 else 0)
        
        # ì‹ ë¢°ë„ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨ ì €ì¥ (ì„ íƒì‚¬í•­)
        if confidences:
            import matplotlib.pyplot as plt
            plt.figure(figsize=(10, 6))
            plt.hist(confidences, bins=20, alpha=0.7)
            plt.title('Detection Confidence Distribution')
            plt.xlabel('Confidence Score')
            plt.ylabel('Frequency')
            
            hist_path = DATA_DIR / f"confidence_hist_{run.info.run_id}.png"
            plt.savefig(hist_path)
            mlflow.log_artifact(str(hist_path))
            plt.close()
        
        print(f"ğŸ‰ ë¶„ì„ ì™„ë£Œ: {len(detected_persons)}ëª… íƒì§€")
        print(f"   - í‰ê·  ì‹ ë¢°ë„: {avg_confidence:.3f}")
        print(f"   - ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ")
        
        # ê¸€ë¡œë²Œ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        performance_metrics["total_videos_processed"] += 1
        performance_metrics["total_persons_detected"] += len(detected_persons)
        performance_metrics["processing_times"].append(processing_time)
        performance_metrics["average_detection_confidence"] = (
            performance_metrics["average_detection_confidence"] + avg_confidence
        ) / 2 if performance_metrics["average_detection_confidence"] > 0 else avg_confidence
        
        return detected_persons, run.info.run_id

def generate_clip_embeddings_with_mlflow(detected_persons, run_id: str = None):
    """MLflow ì¶”ì ì´ í¬í•¨ëœ CLIP ì„ë² ë”© ìƒì„±"""
    
    if not clip_model:
        raise Exception("CLIP ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    with mlflow.start_run(run_id=run_id, nested=True):
        print("ğŸ§  CLIP ì„ë² ë”© ìƒì„± ì¤‘...")
        embedding_start_time = time.time()
        
        embeddings = []
        image_paths = []
        captions = []
        images = []
        embedding_times = []
        
        mlflow.log_param("total_images_to_embed", len(detected_persons))
        
        for i, person in enumerate(detected_persons):
            try:
                embed_start = time.time()
                
                image = person["ì´ë¯¸ì§€"]
                image_input = clip_preprocess(image).unsqueeze(0).to(device)
                
                with torch.no_grad():
                    image_features = clip_model.encode_image(image_input)
                    image_features = image_features.cpu().numpy()[0]
                
                embed_time = time.time() - embed_start
                embedding_times.append(embed_time)
                
                embeddings.append(image_features)
                image_paths.append(person["íŒŒì¼ê²½ë¡œ"])
                
                caption = f"í”„ë ˆì„ {person['í”„ë ˆì„ë²ˆí˜¸']}ì—ì„œ íƒì§€ëœ ì¸ë¬¼ (ì‹ ë¢°ë„: {person['ì‹ ë¢°ë„']:.1%})"
                captions.append(caption)
                
                # Base64 ì¸ì½”ë”©
                buffered = io.BytesIO()
                image.save(buffered, format="JPEG")
                img_base64 = base64.b64encode(buffered.getvalue()).decode()
                images.append(img_base64)
                
            except Exception as e:
                mlflow.log_param(f"embedding_error_{i}", str(e))
                continue
        
        total_embedding_time = time.time() - embedding_start_time
        avg_embedding_time = np.mean(embedding_times) if embedding_times else 0
        
        # ì„ë² ë”© ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¡œê¹…
        mlflow.log_metric("embeddings_generated", len(embeddings))
        mlflow.log_metric("total_embedding_time_seconds", total_embedding_time)
        mlflow.log_metric("average_embedding_time_seconds", avg_embedding_time)
        mlflow.log_metric("embeddings_per_second", len(embeddings) / total_embedding_time if total_embedding_time > 0 else 0)
        
        print(f"âœ… {len(embeddings)}ê°œì˜ ì„ë² ë”© ìƒì„± ì™„ë£Œ")
        print(f"   - ì´ ì„ë² ë”© ì‹œê°„: {total_embedding_time:.2f}ì´ˆ")
        print(f"   - í‰ê·  ì„ë² ë”© ì‹œê°„: {avg_embedding_time:.4f}ì´ˆ")
        
        return {
            "embeddings": np.vstack(embeddings) if embeddings else None,
            "image_paths": image_paths,
            "captions": captions,
            "images": images
        }

@app.post("/upload-video")
async def ì˜ìƒì—…ë¡œë“œ(file: UploadFile = File(...)):
    """MLflow ì¶”ì ì´ í¬í•¨ëœ ì˜ìƒ ì—…ë¡œë“œ ë° ë¶„ì„"""
    
    print(f"ğŸ“¹ MLflow ì¶”ì  ì˜ìƒ ì—…ë¡œë“œ: {file.filename}")
    
    if not file.filename.lower().endswith(('.mp4', '.avi', '.mov', '.mkv')):
        raise HTTPException(status_code=400, detail="ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹")
    
    try:
        # íŒŒì¼ ì €ì¥
        video_path = VIDEO_DIR / file.filename
        with open(video_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        
        # MLflow ì‹¤í—˜ ì‹¤í–‰
        with mlflow.start_run(run_name=f"Video_Analysis_{file.filename}_{datetime.now().strftime('%Y%m%d_%H%M%S')}") as run:
            
            # íŒŒì¼ ì •ë³´ ë¡œê¹…
            mlflow.log_param("filename", file.filename)
            mlflow.log_param("file_size_mb", video_path.stat().st_size / 1024 / 1024)
            mlflow.log_param("upload_timestamp", datetime.now().isoformat())
            
            # ë¹„ë””ì˜¤ ë¶„ì„
            detected_persons, detection_run_id = detect_persons_in_video_with_mlflow(video_path, run.info.run_id)
            
            if not detected_persons:
                mlflow.log_metric("analysis_success", 0)
                return {
                    "status": "success",
                    "message": "ë¶„ì„ ì™„ë£Œ, íƒì§€ëœ ì‚¬ëŒ ì—†ìŒ",
                    "total_crops": 0,
                    "mlflow_run_id": run.info.run_id
                }
            
            # CLIP ì„ë² ë”© ìƒì„±
            embedding_data = generate_clip_embeddings_with_mlflow(detected_persons, run.info.run_id)
            
            # ì „ì²´ ë¶„ì„ ì„±ê³µ ë¡œê¹…
            mlflow.log_metric("analysis_success", 1)
            mlflow.log_metric("total_pipeline_persons", len(detected_persons))
            
            # ê¸€ë¡œë²Œ ë°ì´í„° ì—…ë°ì´íŠ¸
            global analyzed_data
            analyzed_data = embedding_data
            analyzed_data["experiment_id"] = run.info.experiment_id
            analyzed_data["run_id"] = run.info.run_id
            
            return {
                "status": "success",
                "message": f"'{file.filename}' MLflow ì¶”ì  ì™„ë£Œ!",
                "total_crops": len(detected_persons),
                "mlflow_run_id": run.info.run_id,
                "mlflow_experiment_id": run.info.experiment_id,
                "ë¶„ì„ê²°ê³¼": f"{len(detected_persons)}ëª…ì˜ ì‹¤ì œ ì¸ë¬¼ì´ AIë¡œ íƒì§€ë˜ì—ˆìŠµë‹ˆë‹¤"
            }
            
    except Exception as e:
        # ì˜¤ë¥˜ë„ MLflowì— ë¡œê¹…
        with mlflow.start_run():
            mlflow.log_param("error_type", "upload_analysis_error")
            mlflow.log_param("error_message", str(e))
            mlflow.log_metric("analysis_success", 0)
        
        raise HTTPException(status_code=500, detail=f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")

@app.post("/search", response_model=List[SearchResult])
async def ì¸ë¬¼ê²€ìƒ‰(request: SearchRequest):
    """MLflow ì¶”ì ì´ í¬í•¨ëœ CLIP ê¸°ë°˜ ê²€ìƒ‰"""
    
    if analyzed_data["embeddings"] is None:
        raise HTTPException(status_code=404, detail="ë¨¼ì € ì˜ìƒì„ ì—…ë¡œë“œí•˜ê³  ë¶„ì„í•´ì£¼ì„¸ìš”")
    
    # ê²€ìƒ‰ ì‹¤í—˜ ì‹œì‘
    with mlflow.start_run(run_name=f"Search_{request.query[:20]}_{datetime.now().strftime('%H%M%S')}", nested=True):
        
        search_start_time = time.time()
        
        # ê²€ìƒ‰ íŒŒë¼ë¯¸í„° ë¡œê¹…
        mlflow.log_param("search_query", request.query)
        mlflow.log_param("requested_results", request.k)
        mlflow.log_param("available_embeddings", len(analyzed_data["image_paths"]))
        
        try:
            # í…ìŠ¤íŠ¸ ì¸ì½”ë”©
            text_input = clip.tokenize([request.query]).to(device)
            
            with torch.no_grad():
                text_features = clip_model.encode_text(text_input)
                text_features = text_features.cpu().numpy()[0]
            
            # ìœ ì‚¬ë„ ê³„ì‚°
            image_embeddings = analyzed_data["embeddings"]
            similarities = np.dot(image_embeddings, text_features) / (
                np.linalg.norm(image_embeddings, axis=1) * np.linalg.norm(text_features)
            )
            
            # ê²°ê³¼ ì„ íƒ
            top_indices = np.argsort(-similarities)[:request.k]
            
            results = []
            valid_results = 0
            similarity_scores = []
            
            for idx in top_indices:
                similarity = similarities[idx]
                if similarity > current_config["search_similarity_threshold"]:
                    results.append(SearchResult(
                        image_path=analyzed_data["image_paths"][idx],
                        caption=analyzed_data["captions"][idx],
                        score=float(similarity),
                        image_base64=analyzed_data["images"][idx]
                    ))
                    similarity_scores.append(float(similarity))
                    valid_results += 1
            
            search_time = time.time() - search_start_time
            
            # ê²€ìƒ‰ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¡œê¹…
            mlflow.log_metric("search_time_seconds", search_time)
            mlflow.log_metric("results_returned", valid_results)
            mlflow.log_metric("max_similarity", max(similarity_scores) if similarity_scores else 0)
            mlflow.log_metric("avg_similarity", np.mean(similarity_scores) if similarity_scores else 0)
            mlflow.log_metric("min_similarity", min(similarity_scores) if similarity_scores else 0)
            
            # ê²€ìƒ‰ ì„±ê³µë¥  ì—…ë°ì´íŠ¸
            performance_metrics["search_accuracies"].append(max(similarity_scores) if similarity_scores else 0)
            
            print(f"ğŸ” MLflow ì¶”ì  ê²€ìƒ‰ ì™„ë£Œ: {valid_results}ê°œ ê²°ê³¼")
            return results
            
        except Exception as e:
            mlflow.log_param("search_error", str(e))
            mlflow.log_metric("search_success", 0)
            raise HTTPException(status_code=500, detail=f"ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")

@app.get("/mlflow/experiments")
async def mlflow_ì‹¤í—˜ëª©ë¡():
    """MLflow ì‹¤í—˜ ëª©ë¡ ì¡°íšŒ"""
    try:
        client = MlflowClient()
        experiments = client.search_experiments()
        
        experiment_info = []
        for exp in experiments:
            runs = client.search_runs(exp.experiment_id, max_results=10)
            experiment_info.append({
                "experiment_id": exp.experiment_id,
                "name": exp.name,
                "lifecycle_stage": exp.lifecycle_stage,
                "total_runs": len(runs),
                "recent_runs": [
                    {
                        "run_id": run.info.run_id,
                        "run_name": run.data.tags.get("mlflow.runName", "Unnamed"),
                        "start_time": run.info.start_time,
                        "status": run.info.status
                    } for run in runs[:5]
                ]
            })
        
        return {"experiments": experiment_info}
        
    except Exception as e:
        return {"error": f"MLflow ì—°ê²° ì‹¤íŒ¨: {str(e)}"}

@app.post("/config/update")
async def ì„¤ì •ì—…ë°ì´íŠ¸(config: ExperimentConfig):
    """ì‹¤í—˜ ì„¤ì • ì—…ë°ì´íŠ¸"""
    global current_config
    
    with mlflow.start_run(run_name=f"Config_Update_{datetime.now().strftime('%Y%m%d_%H%M%S')}"):
        # ê¸°ì¡´ ì„¤ì • ë¡œê¹…
        for key, value in current_config.items():
            mlflow.log_param(f"old_{key}", value)
        
        # ìƒˆ ì„¤ì • ì ìš© ë° ë¡œê¹…
        current_config.update(config.dict())
        for key, value in current_config.items():
            mlflow.log_param(f"new_{key}", value)
        
        mlflow.log_param("config_update_timestamp", datetime.now().isoformat())
    
    return {
        "status": "success",
        "message": "ì„¤ì •ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤",
        "new_config": current_config
    }

@app.get("/stats/performance")
async def ì„±ëŠ¥í†µê³„():
    """ìƒì„¸ ì„±ëŠ¥ í†µê³„"""
    return {
        "ì „ì²´_ì„±ëŠ¥": performance_metrics,
        "í˜„ì¬_ì„¤ì •": current_config,
        "MLflow_ì •ë³´": {
            "ì‹¤í—˜_ID": analyzed_data.get("experiment_id"),
            "ëŸ°_ID": analyzed_data.get("run_id"),
            "UI_ë§í¬": "http://localhost:5000"
        },
        "ì‹œìŠ¤í…œ_ìƒíƒœ": {
            "YOLO_ë¡œë”©ë¨": yolo_model is not None,
            "CLIP_ë¡œë”©ë¨": clip_model is not None,
            "ë””ë°”ì´ìŠ¤": device,
            "ë¶„ì„ëœ_ë°ì´í„°": len(analyzed_data["image_paths"])
        }
    }

# ì„œë²„ ì‹¤í–‰
if __name__ == "__main__":
    print("ğŸš€ MLflow í†µí•© AI ë¶„ì„ ì„œë²„ ì‹œì‘!")
    print("ğŸ”¬ ì‹¤í—˜ ì¶”ì  ë° ëª¨ë¸ ê´€ë¦¬ í¬í•¨")
    print("=" * 60)
    print("ğŸ“ ì„œë²„: http://localhost:8001")
    print("ğŸ“Š MLflow UI: http://localhost:5000")
    print("ğŸ“š API ë¬¸ì„œ: http://localhost:8001/docs")
    print("ğŸŒ í”„ë¡ íŠ¸ì—”ë“œ: http://localhost:5173")
    print("=" * 60)
    print("ğŸ’¡ MLflow UI ì‹¤í–‰: ë³„ë„ í„°ë¯¸ë„ì—ì„œ 'mlflow ui' ì‹¤í–‰")
    print("=" * 60)
    
    uvicorn.run(app, host="0.0.0.0", port=8001)