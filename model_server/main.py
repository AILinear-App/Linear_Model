# main.py - ì™„ì „ ê¸°ëŠ¥ ë²„ì „ (MLflow + ëª¨ë“  AI ê¸°ëŠ¥)
import os
from fastapi import FastAPI, UploadFile, File, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse
from pydantic import BaseModel
import torch
import clip
import cv2
import numpy as np
import pandas as pd
from PIL import Image
from pathlib import Path
from ultralytics import YOLO
import shutil
from typing import List, Dict, Any
import uvicorn
import base64
import io
import time
import uuid
from datetime import datetime
import logging

# MLflow ê´€ë ¨ imports (ì„ íƒì‚¬í•­)
try:
    import mlflow
    import mlflow.pytorch
    from mlflow.tracking import MlflowClient
    MLFLOW_AVAILABLE = True
    print("âœ… MLflow ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    MLFLOW_AVAILABLE = False
    print("âš ï¸ MLflow ì—†ìŒ - ê¸°ë³¸ ë¡œê¹… ì‚¬ìš©")

# í™˜ê²½ ë³€ìˆ˜
PORT = int(os.getenv("PORT", 8001))
ENVIRONMENT = os.getenv("ENVIRONMENT", "development")

app = FastAPI(
    title="CCTV AI Analysis Server - Full Feature",
    version="3.0.0",
    description="Complete AI server with YOLO + CLIP + MLflow"
)

# CORS ì„¤ì •
if ENVIRONMENT == "production":
    allowed_origins = [
        "https://your-frontend-app.onrender.com",
        "https://your-app.vercel.app",
        "https://localhost:3000"
    ]
else:
    allowed_origins = ["*"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=allowed_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ë°ì´í„° ì €ì¥ í´ë”
ROOT_DIR = Path('./cctv_data')
VIDEO_DIR = ROOT_DIR / 'videos'
CROP_DIR = ROOT_DIR / 'crops'
DATA_DIR = ROOT_DIR / 'data'

# í´ë” ìƒì„±
for folder in [ROOT_DIR, VIDEO_DIR, CROP_DIR, DATA_DIR]:
    folder.mkdir(parents=True, exist_ok=True)

# ì „ì—­ ë³€ìˆ˜
device = 'cuda' if torch.cuda.is_available() else 'cpu'
yolo_model = None
clip_model = None
clip_preprocess = None

# MLflow ì„¤ì •
if MLFLOW_AVAILABLE:
    mlflow.set_experiment("CCTV_Person_Detection")

# í˜„ì¬ ì‹¤í—˜ ì„¤ì •
current_config = {
    "yolo_confidence_threshold": 0.5,
    "yolo_model_size": "yolov8n",
    "clip_model_type": "ViT-B/32",
    "max_frames_per_video": 30,
    "min_person_size": 50,
    "search_similarity_threshold": 0.1
}

# ìš”ì²­/ì‘ë‹µ ëª¨ë¸
class SearchRequest(BaseModel):
    query: str
    k: int = 5

class SearchResult(BaseModel):
    image_path: str
    caption: str
    score: float
    image_base64: str = ""

class ExperimentConfig(BaseModel):
    yolo_confidence_threshold: float = 0.5
    yolo_model_size: str = "yolov8n"
    max_frames_per_video: int = 30
    min_person_size: int = 50

# ë¶„ì„ëœ ë°ì´í„° ì €ì¥
analyzed_data = {
    "embeddings": None,
    "image_paths": [],
    "captions": [],
    "images": [],
    "experiment_id": None,
    "run_id": None
}

# ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì €ì¥
performance_metrics = {
    "total_videos_processed": 0,
    "total_persons_detected": 0,
    "average_detection_confidence": 0.0,
    "processing_times": [],
    "search_accuracies": []
}

@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ì‹œ AI ëª¨ë¸ ë¡œë“œ"""
    global yolo_model, clip_model, clip_preprocess
    
    try:
        print("ğŸš€ Complete AI ì„œë²„ ì‹œì‘!")
        print(f"ğŸŒ í™˜ê²½: {ENVIRONMENT}")
        print(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {device}")
        print(f"ğŸ“Š MLflow: {'ì‚¬ìš©' if MLFLOW_AVAILABLE else 'ë¯¸ì‚¬ìš©'}")
        
        # MLflow ì‹¤í—˜ ì‹œì‘
        run_info = None
        if MLFLOW_AVAILABLE:
            run = mlflow.start_run(run_name=f"Server_Startup_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
            run_info = run.info
            
            # ì‹œìŠ¤í…œ ì •ë³´ ë¡œê¹…
            mlflow.log_param("device", device)
            mlflow.log_param("environment", ENVIRONMENT)
            for key, value in current_config.items():
                mlflow.log_param(key, value)
        
        print("ğŸ¤– AI ëª¨ë¸ ë¡œë”© ì¤‘...")
        start_time = time.time()
        
        # YOLO ëª¨ë¸ ë¡œë“œ
        print(f"ğŸ“¦ YOLO ëª¨ë¸ ë¡œë”©: {current_config['yolo_model_size']}")
        yolo_model = YOLO(f"{current_config['yolo_model_size']}.pt")
        yolo_load_time = time.time() - start_time
        
        # CLIP ëª¨ë¸ ë¡œë“œ
        print(f"ğŸ§  CLIP ëª¨ë¸ ë¡œë”©: {current_config['clip_model_type']}")
        clip_start = time.time()
        clip_model, clip_preprocess = clip.load(current_config['clip_model_type'], device=device)
        clip_load_time = time.time() - clip_start
        
        total_load_time = time.time() - start_time
        
        # ë¡œë”© ì‹œê°„ ë¡œê¹…
        if MLFLOW_AVAILABLE:
            mlflow.log_metric("yolo_load_time_seconds", yolo_load_time)
            mlflow.log_metric("clip_load_time_seconds", clip_load_time)
            mlflow.log_metric("total_load_time_seconds", total_load_time)
            mlflow.log_param("yolo_model_loaded", True)
            mlflow.log_param("clip_model_loaded", True)
        
        print("âœ… ëª¨ë“  AI ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
        print(f"   - YOLO ë¡œë”© ì‹œê°„: {yolo_load_time:.2f}ì´ˆ")
        print(f"   - CLIP ë¡œë”© ì‹œê°„: {clip_load_time:.2f}ì´ˆ")
        print(f"   - ì´ ë¡œë”© ì‹œê°„: {total_load_time:.2f}ì´ˆ")
        
        if MLFLOW_AVAILABLE:
            mlflow.end_run()
        
    except Exception as e:
        print(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        if MLFLOW_AVAILABLE:
            mlflow.log_param("initialization_error", str(e))
            mlflow.end_run()

@app.get("/")
async def root():
    """ë©”ì¸ í˜ì´ì§€"""
    return {
        "service": "CCTV AI Analysis Server - Full Feature",
        "version": "3.0.0",
        "environment": ENVIRONMENT,
        "mlflow_enabled": MLFLOW_AVAILABLE,
        "models": {
            "yolo": yolo_model is not None,
            "clip": clip_model is not None
        },
        "config": current_config,
        "stats": performance_metrics,
        "message": "âœ… ì™„ì „ ê¸°ëŠ¥ ì„œë²„ ì‹¤í–‰ì¤‘!"
    }

@app.get("/health")
async def health_check():
    """ìƒì„¸ í—¬ìŠ¤ì²´í¬"""
    return {
        "status": "healthy",
        "models": {
            "yolo": yolo_model is not None,
            "clip": clip_model is not None
        },
        "mlflow": MLFLOW_AVAILABLE,
        "device": device,
        "analyzed_data": len(analyzed_data["image_paths"]),
        "performance": performance_metrics
    }

def detect_persons_in_video(video_path: Path, max_frames: int = 30):
    """ë¹„ë””ì˜¤ì—ì„œ ì‚¬ëŒ íƒì§€"""
    
    if not yolo_model:
        raise Exception("YOLO ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    print(f"ğŸ¬ ë¹„ë””ì˜¤ ë¶„ì„ ì‹œì‘: {video_path.name}")
    
    cap = cv2.VideoCapture(str(video_path))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    
    detected_persons = []
    frame_count = 0
    
    # í”„ë ˆì„ ê°„ê²© ê³„ì‚°
    frame_interval = max(1, total_frames // max_frames)
    
    print(f"ğŸ“Š ì´ í”„ë ˆì„: {total_frames}, ë¶„ì„í•  í”„ë ˆì„: {min(max_frames, total_frames)}")
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
            
        frame_count += 1
        
        if frame_count % frame_interval != 0:
            continue
            
        try:
            # YOLO ì¶”ë¡ 
            results = yolo_model(frame, classes=[0], verbose=False)
            
            for r in results:
                boxes = r.boxes
                if boxes is not None:
                    for box in boxes:
                        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                        confidence = box.conf[0].cpu().numpy()
                        
                        if confidence > current_config["yolo_confidence_threshold"]:
                            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)
                            
                            if (x2 - x1) > current_config["min_person_size"] and (y2 - y1) > current_config["min_person_size"]:
                                person_img = frame[y1:y2, x1:x2]
                                person_pil = Image.fromarray(cv2.cvtColor(person_img, cv2.COLOR_BGR2RGB))
                                
                                filename = f"{video_path.stem}_frame{frame_count}_person{len(detected_persons)}.jpg"
                                crop_path = CROP_DIR / filename
                                person_pil.save(crop_path)
                                
                                detected_persons.append({
                                    "íŒŒì¼ê²½ë¡œ": str(crop_path),
                                    "í”„ë ˆì„ë²ˆí˜¸": frame_count,
                                    "ì‹ ë¢°ë„": float(confidence),
                                    "ë°•ìŠ¤ì¢Œí‘œ": [x1, y1, x2, y2],
                                    "ì´ë¯¸ì§€": person_pil
                                })
                                
                                print(f"âœ… ì‚¬ëŒ íƒì§€: í”„ë ˆì„ {frame_count}, ì‹ ë¢°ë„ {confidence:.2f}")
        
        except Exception as e:
            print(f"âš ï¸ í”„ë ˆì„ {frame_count} ë¶„ì„ ì‹¤íŒ¨: {e}")
            continue
    
    cap.release()
    print(f"ğŸ‰ ë¶„ì„ ì™„ë£Œ: {len(detected_persons)}ëª…ì˜ ì‚¬ëŒ íƒì§€")
    
    return detected_persons

def generate_clip_embeddings(detected_persons):
    """CLIP ì„ë² ë”© ìƒì„±"""
    
    if not clip_model:
        raise Exception("CLIP ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    print("ğŸ§  CLIP ì„ë² ë”© ìƒì„± ì¤‘...")
    
    embeddings = []
    image_paths = []
    captions = []
    images = []
    
    for i, person in enumerate(detected_persons):
        try:
            image = person["ì´ë¯¸ì§€"]
            image_input = clip_preprocess(image).unsqueeze(0).to(device)
            
            with torch.no_grad():
                image_features = clip_model.encode_image(image_input)
                image_features = image_features.cpu().numpy()[0]
            
            embeddings.append(image_features)
            image_paths.append(person["íŒŒì¼ê²½ë¡œ"])
            
            caption = f"í”„ë ˆì„ {person['í”„ë ˆì„ë²ˆí˜¸']}ì—ì„œ íƒì§€ëœ ì¸ë¬¼ (ì‹ ë¢°ë„: {person['ì‹ ë¢°ë„']:.1%})"
            captions.append(caption)
            
            # Base64 ì¸ì½”ë”©
            buffered = io.BytesIO()
            image.save(buffered, format="JPEG")
            img_base64 = base64.b64encode(buffered.getvalue()).decode()
            images.append(img_base64)
            
        except Exception as e:
            print(f"âš ï¸ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨ {i}: {e}")
            continue
    
    print(f"âœ… {len(embeddings)}ê°œì˜ ì„ë² ë”© ìƒì„± ì™„ë£Œ")
    
    return {
        "embeddings": np.vstack(embeddings) if embeddings else None,
        "image_paths": image_paths,
        "captions": captions,
        "images": images
    }

@app.post("/upload-video")
async def upload_video(file: UploadFile = File(...)):
    """ì˜ìƒ ì—…ë¡œë“œ ë° ì™„ì „ AI ë¶„ì„"""
    
    print(f"ğŸ“¹ ì™„ì „ AI ë¶„ì„ ì‹œì‘: {file.filename}")
    
    if not file.filename.lower().endswith(('.mp4', '.avi', '.mov', '.mkv')):
        raise HTTPException(status_code=400, detail="ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹")
    
    # MLflow ì‹¤í—˜ ì‹œì‘
    mlflow_run_id = None
    mlflow_experiment_id = None
    
    if MLFLOW_AVAILABLE:
        run = mlflow.start_run(run_name=f"Video_Analysis_{file.filename}_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
        mlflow_run_id = run.info.run_id
        mlflow_experiment_id = run.info.experiment_id
        
        mlflow.log_param("filename", file.filename)
        mlflow.log_param("upload_timestamp", datetime.now().isoformat())
    
    try:
        # íŒŒì¼ ì €ì¥
        video_path = VIDEO_DIR / file.filename
        with open(video_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        
        if MLFLOW_AVAILABLE:
            mlflow.log_param("file_size_mb", video_path.stat().st_size / 1024 / 1024)
        
        # ì‚¬ëŒ íƒì§€
        detected_persons = detect_persons_in_video(video_path, current_config["max_frames_per_video"])
        
        if not detected_persons:
            if MLFLOW_AVAILABLE:
                mlflow.log_metric("persons_detected", 0)
                mlflow.log_metric("analysis_success", 1)
                mlflow.end_run()
            
            return {
                "status": "success",
                "message": "ë¶„ì„ ì™„ë£Œ, íƒì§€ëœ ì‚¬ëŒ ì—†ìŒ",
                "total_crops": 0,
                "mlflow_run_id": mlflow_run_id,
                "mlflow_experiment_id": mlflow_experiment_id
            }
        
        # CLIP ì„ë² ë”© ìƒì„±
        embedding_data = generate_clip_embeddings(detected_persons)
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        performance_metrics["total_videos_processed"] += 1
        performance_metrics["total_persons_detected"] += len(detected_persons)
        avg_confidence = sum(p["ì‹ ë¢°ë„"] for p in detected_persons) / len(detected_persons)
        performance_metrics["average_detection_confidence"] = avg_confidence
        
        if MLFLOW_AVAILABLE:
            mlflow.log_metric("persons_detected", len(detected_persons))
            mlflow.log_metric("average_confidence", avg_confidence)
            mlflow.log_metric("analysis_success", 1)
            mlflow.end_run()
        
        # ê¸€ë¡œë²Œ ë°ì´í„° ì—…ë°ì´íŠ¸
        global analyzed_data
        analyzed_data = embedding_data
        analyzed_data["experiment_id"] = mlflow_experiment_id
        analyzed_data["run_id"] = mlflow_run_id
        
        return {
            "status": "success",
            "message": f"'{file.filename}' ì™„ì „ AI ë¶„ì„ ì™„ë£Œ!",
            "total_crops": len(detected_persons),
            "mlflow_run_id": mlflow_run_id,
            "mlflow_experiment_id": mlflow_experiment_id,
            "ë¶„ì„ê²°ê³¼": f"{len(detected_persons)}ëª…ì˜ ì‹¤ì œ ì¸ë¬¼ì´ AIë¡œ íƒì§€ë˜ì—ˆìŠµë‹ˆë‹¤"
        }
        
    except Exception as e:
        if MLFLOW_AVAILABLE:
            mlflow.log_param("error", str(e))
            mlflow.log_metric("analysis_success", 0)
            mlflow.end_run()
        
        raise HTTPException(status_code=500, detail=f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")

@app.post("/search", response_model=List[SearchResult])
async def search_persons(request: SearchRequest):
    """ì™„ì „ CLIP ê¸°ë°˜ ê²€ìƒ‰"""
    
    if analyzed_data["embeddings"] is None:
        raise HTTPException(status_code=404, detail="ë¨¼ì € ì˜ìƒì„ ì—…ë¡œë“œí•˜ê³  ë¶„ì„í•´ì£¼ì„¸ìš”")
    
    # MLflow ê²€ìƒ‰ ì‹¤í—˜ ì‹œì‘
    if MLFLOW_AVAILABLE:
        search_run = mlflow.start_run(run_name=f"Search_{request.query[:20]}", nested=True)
        mlflow.log_param("search_query", request.query)
        mlflow.log_param("requested_results", request.k)
    
    try:
        # CLIP í…ìŠ¤íŠ¸ ì¸ì½”ë”©
        text_input = clip.tokenize([request.query]).to(device)
        
        with torch.no_grad():
            text_features = clip_model.encode_text(text_input)
            text_features = text_features.cpu().numpy()[0]
        
        # ìœ ì‚¬ë„ ê³„ì‚°
        image_embeddings = analyzed_data["embeddings"]
        similarities = np.dot(image_embeddings, text_features) / (
            np.linalg.norm(image_embeddings, axis=1) * np.linalg.norm(text_features)
        )
        
        # ìƒìœ„ ê²°ê³¼ ì„ íƒ
        top_indices = np.argsort(-similarities)[:request.k]
        
        results = []
        valid_results = 0
        
        for idx in top_indices:
            similarity = similarities[idx]
            if similarity > current_config["search_similarity_threshold"]:
                results.append(SearchResult(
                    image_path=analyzed_data["image_paths"][idx],
                    caption=analyzed_data["captions"][idx],
                    score=float(similarity),
                    image_base64=analyzed_data["images"][idx]
                ))
                valid_results += 1
        
        if MLFLOW_AVAILABLE:
            mlflow.log_metric("results_returned", valid_results)
            mlflow.log_metric("max_similarity", max(similarities) if len(similarities) > 0 else 0)
            mlflow.end_run()
        
        print(f"ğŸ” ì™„ì „ AI ê²€ìƒ‰ ì™„ë£Œ: {valid_results}ê°œ ê²°ê³¼")
        return results
        
    except Exception as e:
        if MLFLOW_AVAILABLE:
            mlflow.log_param("search_error", str(e))
            mlflow.end_run()
        raise HTTPException(status_code=500, detail=f"ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")

@app.get("/stats/performance")
async def get_performance_stats():
    """ìƒì„¸ ì„±ëŠ¥ í†µê³„"""
    return {
        "ì „ì²´_ì„±ëŠ¥": performance_metrics,
        "í˜„ì¬_ì„¤ì •": current_config,
        "MLflow_ì •ë³´": {
            "ì‚¬ìš©ê°€ëŠ¥": MLFLOW_AVAILABLE,
            "ì‹¤í—˜_ID": analyzed_data.get("experiment_id"),
            "ëŸ°_ID": analyzed_data.get("run_id"),
            "UI_ë§í¬": "http://localhost:5000" if MLFLOW_AVAILABLE else "MLflow ë¯¸ì‚¬ìš©"
        },
        "ì‹œìŠ¤í…œ_ìƒíƒœ": {
            "YOLO_ë¡œë”©ë¨": yolo_model is not None,
            "CLIP_ë¡œë”©ë¨": clip_model is not None,
            "ë””ë°”ì´ìŠ¤": device,
            "ë¶„ì„ëœ_ë°ì´í„°": len(analyzed_data["image_paths"])
        }
    }

@app.get("/mlflow/experiments")
async def get_mlflow_experiments():
    """MLflow ì‹¤í—˜ ëª©ë¡"""
    if not MLFLOW_AVAILABLE:
        return {"error": "MLflowê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"}
    
    try:
        client = MlflowClient()
        experiments = client.search_experiments()
        
        experiment_info = []
        for exp in experiments:
            runs = client.search_runs(exp.experiment_id, max_results=10)
            experiment_info.append({
                "experiment_id": exp.experiment_id,
                "name": exp.name,
                "lifecycle_stage": exp.lifecycle_stage,
                "total_runs": len(runs),
                "recent_runs": [
                    {
                        "run_id": run.info.run_id,
                        "run_name": run.data.tags.get("mlflow.runName", "Unnamed"),
                        "start_time": run.info.start_time,
                        "status": run.info.status
                    } for run in runs[:5]
                ]
            })
        
        return {"experiments": experiment_info}
        
    except Exception as e:
        return {"error": f"MLflow ì—°ê²° ì‹¤íŒ¨: {str(e)}"}

@app.get("/image/{filename}")
async def get_image(filename: str):
    """ì´ë¯¸ì§€ íŒŒì¼ ë°˜í™˜"""
    image_path = CROP_DIR / filename
    if image_path.exists():
        return FileResponse(image_path)
    else:
        raise HTTPException(status_code=404, detail="ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

# ì„œë²„ ì‹¤í–‰
if __name__ == "__main__":
    print("ğŸš€ ì™„ì „ ê¸°ëŠ¥ AI ë¶„ì„ ì„œë²„ ì‹œì‘!")
    print("ğŸ¤– YOLO + CLIP + MLflow ëª¨ë“  ê¸°ëŠ¥ í¬í•¨")
    print("=" * 60)
    print(f"ğŸ“ ì„œë²„: http://localhost:{PORT}")
    print(f"ğŸ“Š MLflow: {'ì‚¬ìš©' if MLFLOW_AVAILABLE else 'ë¯¸ì‚¬ìš©'}")
    print("=" * 60)
    
    uvicorn.run(app, host="0.0.0.0", port=PORT)