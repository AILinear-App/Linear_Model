# main.py - ì‹¤ì œ AIê°€ í¬í•¨ëœ ì„œë²„ (YOLO + CLIP)
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse
from pydantic import BaseModel
import torch
import clip
import cv2
import numpy as np
import pandas as pd
from PIL import Image, ImageDraw, ImageFont
from pathlib import Path
from ultralytics import YOLO
import shutil
from typing import List
import uvicorn
import json
import base64
import io

# FastAPI ì•± ìƒì„±
app = FastAPI(title="CCTV AI ë¶„ì„ ì„œë²„ (ì‹¤ì œ AI)", version="2.0.0")

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ë°ì´í„° ì €ì¥ í´ë”
ROOT_DIR = Path('./cctv_data')
VIDEO_DIR = ROOT_DIR / 'videos'
CROP_DIR = ROOT_DIR / 'crops'
DATA_DIR = ROOT_DIR / 'data'

# í´ë” ìƒì„±
for folder in [ROOT_DIR, VIDEO_DIR, CROP_DIR, DATA_DIR]:
    folder.mkdir(parents=True, exist_ok=True)

# ì „ì—­ ë³€ìˆ˜ (AI ëª¨ë¸ë“¤)
device = 'cuda' if torch.cuda.is_available() else 'cpu'
yolo_model = None
clip_model = None
clip_preprocess = None

print(f"ğŸ”§ ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {device}")

# ìš”ì²­/ì‘ë‹µ ëª¨ë¸
class SearchRequest(BaseModel):
    query: str
    k: int = 5

class SearchResult(BaseModel):
    image_path: str
    caption: str
    score: float
    image_base64: str = ""

# ë¶„ì„ëœ ë°ì´í„° ì €ì¥
analyzed_data = {
    "embeddings": None,
    "image_paths": [],
    "captions": [],
    "images": []
}

@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ì‹œ AI ëª¨ë¸ ë¡œë“œ"""
    global yolo_model, clip_model, clip_preprocess
    
    try:
        print("ğŸ¤– AI ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        # YOLO ëª¨ë¸ ë¡œë“œ (ì‚¬ëŒ íƒì§€ìš©)
        print("ğŸ“¦ YOLO ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘...")
        yolo_model = YOLO('yolov8n.pt')  # nano ë²„ì „ (ë¹ ë¥´ê³  ê°€ë²¼ì›€)
        
        # CLIP ëª¨ë¸ ë¡œë“œ (ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ë§¤ì¹­ìš©)
        print("ğŸ§  CLIP ëª¨ë¸ ë¡œë”© ì¤‘...")
        clip_model, clip_preprocess = clip.load("ViT-B/32", device=device)
        
        print("âœ… ëª¨ë“  AI ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
        print(f"   - YOLO: ì‚¬ëŒ íƒì§€ ì¤€ë¹„ì™„ë£Œ")
        print(f"   - CLIP: ìì—°ì–´ ê²€ìƒ‰ ì¤€ë¹„ì™„ë£Œ")
        print(f"   - ë””ë°”ì´ìŠ¤: {device}")
        
    except Exception as e:
        print(f"âŒ AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        print("âš ï¸  ê¸°ë³¸ ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤")

@app.get("/")
async def ë©”ì¸í˜ì´ì§€():
    """ì„œë²„ ë©”ì¸ í˜ì´ì§€"""
    return {
        "ë©”ì‹œì§€": "ğŸ¤– ì‹¤ì œ AI ë¶„ì„ ì„œë²„ê°€ ì‹¤í–‰ì¤‘ì…ë‹ˆë‹¤!",
        "YOLO_ëª¨ë¸": "ë¡œë”©ë¨" if yolo_model else "ë¡œë”©ì‹¤íŒ¨",
        "CLIP_ëª¨ë¸": "ë¡œë”©ë¨" if clip_model else "ë¡œë”©ì‹¤íŒ¨",
        "ë””ë°”ì´ìŠ¤": device,
        "ë¶„ì„ëœ_ì´ë¯¸ì§€": len(analyzed_data["image_paths"])
    }

@app.get("/health")
async def ìƒíƒœí™•ì¸():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    return {
        "ìƒíƒœ": "ì •ìƒ",
        "AI_ì¤€ë¹„": all([yolo_model, clip_model]),
        "ë””ë°”ì´ìŠ¤": device,
        "ë¶„ì„ëœ_ë°ì´í„°": len(analyzed_data["image_paths"])
    }

def detect_persons_in_video(video_path: Path, max_frames: int = 30):
    """ë¹„ë””ì˜¤ì—ì„œ ì‚¬ëŒ íƒì§€ ë° crop ì´ë¯¸ì§€ ìƒì„±"""
    
    if not yolo_model:
        raise Exception("YOLO ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    print(f"ğŸ¬ ë¹„ë””ì˜¤ ë¶„ì„ ì‹œì‘: {video_path.name}")
    
    cap = cv2.VideoCapture(str(video_path))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    
    detected_persons = []
    frame_count = 0
    
    # í”„ë ˆì„ ê°„ê²© ê³„ì‚° (ì „ì²´ ì˜ìƒì—ì„œ max_framesê°œë§Œ ë¶„ì„)
    frame_interval = max(1, total_frames // max_frames)
    
    print(f"ğŸ“Š ì´ í”„ë ˆì„: {total_frames}, ë¶„ì„í•  í”„ë ˆì„: {min(max_frames, total_frames)}")
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
            
        frame_count += 1
        
        # ì§€ì •ëœ ê°„ê²©ìœ¼ë¡œë§Œ ë¶„ì„
        if frame_count % frame_interval != 0:
            continue
            
        try:
            # YOLOë¡œ ì‚¬ëŒ íƒì§€
            results = yolo_model(frame, classes=[0], verbose=False)  # class 0 = person
            
            for r in results:
                boxes = r.boxes
                if boxes is not None:
                    for box in boxes:
                        # ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ
                        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                        confidence = box.conf[0].cpu().numpy()
                        
                        # ì‹ ë¢°ë„ê°€ 0.5 ì´ìƒì¸ ê²½ìš°ë§Œ
                        if confidence > 0.5:
                            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)
                            
                            # ë„ˆë¬´ ì‘ì€ ë°•ìŠ¤ ì œì™¸
                            if (x2 - x1) > 50 and (y2 - y1) > 50:
                                # ì‚¬ëŒ ì˜ì—­ crop
                                person_img = frame[y1:y2, x1:x2]
                                
                                # ì´ë¯¸ì§€ë¥¼ PILë¡œ ë³€í™˜
                                person_pil = Image.fromarray(cv2.cvtColor(person_img, cv2.COLOR_BGR2RGB))
                                
                                # íŒŒì¼ëª… ìƒì„±
                                filename = f"{video_path.stem}_frame{frame_count}_person{len(detected_persons)}.jpg"
                                crop_path = CROP_DIR / filename
                                
                                # ì´ë¯¸ì§€ ì €ì¥
                                person_pil.save(crop_path)
                                
                                detected_persons.append({
                                    "íŒŒì¼ê²½ë¡œ": str(crop_path),
                                    "í”„ë ˆì„ë²ˆí˜¸": frame_count,
                                    "ì‹ ë¢°ë„": float(confidence),
                                    "ë°•ìŠ¤ì¢Œí‘œ": [x1, y1, x2, y2],
                                    "ì´ë¯¸ì§€": person_pil
                                })
                                
                                print(f"âœ… ì‚¬ëŒ íƒì§€: í”„ë ˆì„ {frame_count}, ì‹ ë¢°ë„ {confidence:.2f}")
        
        except Exception as e:
            print(f"âš ï¸ í”„ë ˆì„ {frame_count} ë¶„ì„ ì‹¤íŒ¨: {e}")
            continue
    
    cap.release()
    print(f"ğŸ‰ ë¶„ì„ ì™„ë£Œ: {len(detected_persons)}ëª…ì˜ ì‚¬ëŒ íƒì§€")
    
    return detected_persons

def generate_clip_embeddings(detected_persons):
    """íƒì§€ëœ ì‚¬ëŒë“¤ì˜ CLIP ì„ë² ë”© ìƒì„±"""
    
    if not clip_model:
        raise Exception("CLIP ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    print("ğŸ§  CLIP ì„ë² ë”© ìƒì„± ì¤‘...")
    
    embeddings = []
    image_paths = []
    captions = []
    images = []
    
    for i, person in enumerate(detected_persons):
        try:
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            image = person["ì´ë¯¸ì§€"]
            image_input = clip_preprocess(image).unsqueeze(0).to(device)
            
            # CLIP ì„ë² ë”© ìƒì„±
            with torch.no_grad():
                image_features = clip_model.encode_image(image_input)
                image_features = image_features.cpu().numpy()[0]
            
            embeddings.append(image_features)
            image_paths.append(person["íŒŒì¼ê²½ë¡œ"])
            
            # ê¸°ë³¸ ìº¡ì…˜ ìƒì„±
            caption = f"í”„ë ˆì„ {person['í”„ë ˆì„ë²ˆí˜¸']}ì—ì„œ íƒì§€ëœ ì¸ë¬¼ (ì‹ ë¢°ë„: {person['ì‹ ë¢°ë„']:.1%})"
            captions.append(caption)
            
            # Base64 ì¸ì½”ë”©ì„ ìœ„í•´ ì´ë¯¸ì§€ ì €ì¥
            buffered = io.BytesIO()
            image.save(buffered, format="JPEG")
            img_base64 = base64.b64encode(buffered.getvalue()).decode()
            images.append(img_base64)
            
        except Exception as e:
            print(f"âš ï¸ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨ {i}: {e}")
            continue
    
    print(f"âœ… {len(embeddings)}ê°œì˜ ì„ë² ë”© ìƒì„± ì™„ë£Œ")
    
    return {
        "embeddings": np.vstack(embeddings) if embeddings else None,
        "image_paths": image_paths,
        "captions": captions,
        "images": images
    }

@app.post("/upload-video")
async def ì˜ìƒì—…ë¡œë“œ(file: UploadFile = File(...)):
    """CCTV ì˜ìƒ ì—…ë¡œë“œ ë° ì‹¤ì œ AI ë¶„ì„"""
    
    print(f"ğŸ“¹ ì—…ë¡œë“œ ì‹œì‘: {file.filename}")
    
    # íŒŒì¼ í˜•ì‹ ì²´í¬
    if not file.filename.lower().endswith(('.mp4', '.avi', '.mov', '.mkv')):
        raise HTTPException(status_code=400, detail="ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤")
    
    try:
        # íŒŒì¼ ì €ì¥
        video_path = VIDEO_DIR / file.filename
        with open(video_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        
        print(f"ğŸ’¾ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {video_path}")
        
        # ì‹¤ì œ AI ë¶„ì„ ìˆ˜í–‰
        if not yolo_model or not clip_model:
            return {
                "status": "warning",
                "message": "AI ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•„ ê°€ì§œ ë°ì´í„°ë¡œ ì‘ë‹µí•©ë‹ˆë‹¤",
                "total_crops": 5
            }
        
        # 1. YOLOë¡œ ì‚¬ëŒ íƒì§€
        detected_persons = detect_persons_in_video(video_path, max_frames=20)
        
        if not detected_persons:
            return {
                "status": "success",
                "message": "ì˜ìƒ ë¶„ì„ ì™„ë£Œ, í•˜ì§€ë§Œ íƒì§€ëœ ì‚¬ëŒì´ ì—†ìŠµë‹ˆë‹¤",
                "total_crops": 0
            }
        
        # 2. CLIP ì„ë² ë”© ìƒì„±
        embedding_data = generate_clip_embeddings(detected_persons)
        
        # 3. ê¸€ë¡œë²Œ ë°ì´í„°ì— ì €ì¥
        global analyzed_data
        analyzed_data = embedding_data
        
        return {
            "status": "success", 
            "message": f"'{file.filename}' ë¶„ì„ ì™„ë£Œ!",
            "total_crops": len(detected_persons),
            "ë¶„ì„ê²°ê³¼": f"{len(detected_persons)}ëª…ì˜ ì‹¤ì œ ì¸ë¬¼ì´ AIë¡œ íƒì§€ë˜ì—ˆìŠµë‹ˆë‹¤"
        }
        
    except Exception as e:
        print(f"âŒ ë¶„ì„ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")

@app.post("/search", response_model=List[SearchResult])
async def ì¸ë¬¼ê²€ìƒ‰(request: SearchRequest):
    """ì‹¤ì œ CLIP ê¸°ë°˜ ìì—°ì–´ ê²€ìƒ‰"""
    
    print(f"ğŸ” ì‹¤ì œ AI ê²€ìƒ‰: '{request.query}'")
    
    if not analyzed_data["embeddings"] is not None:
        raise HTTPException(status_code=404, detail="ë¨¼ì € ì˜ìƒì„ ì—…ë¡œë“œí•˜ê³  ë¶„ì„í•´ì£¼ì„¸ìš”")
    
    if not clip_model:
        raise HTTPException(status_code=500, detail="CLIP ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    try:
        # ê²€ìƒ‰ì–´ë¥¼ CLIPìœ¼ë¡œ ì¸ì½”ë”©
        text_input = clip.tokenize([request.query]).to(device)
        
        with torch.no_grad():
            text_features = clip_model.encode_text(text_input)
            text_features = text_features.cpu().numpy()[0]
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        image_embeddings = analyzed_data["embeddings"]
        similarities = np.dot(image_embeddings, text_features) / (
            np.linalg.norm(image_embeddings, axis=1) * np.linalg.norm(text_features)
        )
        
        # ìƒìœ„ kê°œ ê²°ê³¼ ì„ íƒ
        top_indices = np.argsort(-similarities)[:request.k]
        
        results = []
        for idx in top_indices:
            if similarities[idx] > 0.1:  # ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’
                results.append(SearchResult(
                    image_path=analyzed_data["image_paths"][idx],
                    caption=analyzed_data["captions"][idx],
                    score=float(similarities[idx]),
                    image_base64=analyzed_data["images"][idx]
                ))
        
        print(f"âœ… ì‹¤ì œ AI ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼")
        return results
        
    except Exception as e:
        print(f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")

@app.get("/image/{filename}")
async def get_image(filename: str):
    """crop ì´ë¯¸ì§€ íŒŒì¼ ë°˜í™˜"""
    image_path = CROP_DIR / filename
    if image_path.exists():
        return FileResponse(image_path)
    else:
        raise HTTPException(status_code=404, detail="ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

@app.get("/stats")
async def í†µê³„():
    """ì„œë²„ í†µê³„"""
    return {
        "AI_ëª¨ë¸_ìƒíƒœ": {
            "YOLO": "ë¡œë”©ë¨" if yolo_model else "ë¡œë”©ì‹¤íŒ¨",
            "CLIP": "ë¡œë”©ë¨" if clip_model else "ë¡œë”©ì‹¤íŒ¨"
        },
        "ë¶„ì„_í†µê³„": {
            "íƒì§€ëœ_ì¸ë¬¼": len(analyzed_data["image_paths"]),
            "ì„ë² ë”©_ìƒì„±ë¨": analyzed_data["embeddings"] is not None
        },
        "ì‹œìŠ¤í…œ_ì •ë³´": {
            "ë””ë°”ì´ìŠ¤": device,
            "GPU_ì‚¬ìš©ê°€ëŠ¥": torch.cuda.is_available()
        }
    }

# ì„œë²„ ì‹¤í–‰
if __name__ == "__main__":
    print("ğŸš€ ì‹¤ì œ AI ë¶„ì„ ì„œë²„ ì‹œì‘!")
    print("ğŸ¤– YOLO + CLIP ê¸°ë°˜ ì¸ë¬¼ íƒì§€ ë° ê²€ìƒ‰")
    print("ğŸ“ ì„œë²„: http://localhost:8001")
    print("ğŸ“š API ë¬¸ì„œ: http://localhost:8001/docs")
    print("=" * 60)
    
    uvicorn.run(app, host="0.0.0.0", port=8001)