# main.py - MLflow í†µí•©ëœ ì‹¤ì œ AI ì„œë²„
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse
from pydantic import BaseModel
import torch
import clip
import cv2
import numpy as np
import pandas as pd
from PIL import Image, ImageDraw, ImageFont
from pathlib import Path
from ultralytics import YOLO
import shutil
from typing import List
import uvicorn
import json
import base64
import io
import time
from datetime import datetime

# MLflow ì„í¬íŠ¸
import mlflow
import mlflow.pytorch
import mlflow.sklearn
from mlflow.tracking import MlflowClient

# FastAPI ì•± ìƒì„±
app = FastAPI(title="CCTV AI ë¶„ì„ ì„œë²„ + MLflow", version="3.0.0")

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# MLflow ì„¤ì •
mlflow.set_tracking_uri("http://localhost:5000")
mlflow.set_experiment("CCTV_AI_Analysis")

# ë°ì´í„° ì €ì¥ í´ë”
ROOT_DIR = Path('./cctv_data')
VIDEO_DIR = ROOT_DIR / 'videos'
CROP_DIR = ROOT_DIR / 'crops'
DATA_DIR = ROOT_DIR / 'data'

# í´ë” ìƒì„±
for folder in [ROOT_DIR, VIDEO_DIR, CROP_DIR, DATA_DIR]:
    folder.mkdir(parents=True, exist_ok=True)

# ì „ì—­ ë³€ìˆ˜
device = 'cuda' if torch.cuda.is_available() else 'cpu'
yolo_model = None
clip_model = None
clip_preprocess = None

# ì„±ëŠ¥ í†µê³„ ì €ì¥
performance_stats = {
    "total_videos_processed": 0,
    "total_persons_detected": 0,
    "total_searches_performed": 0,
    "average_detection_confidence": 0.0,
    "processing_times": []
}

# í˜„ì¬ MLflow ì‹¤í–‰ ì •ë³´
current_mlflow_info = {
    "run_id": None,
    "experiment_id": None
}

print(f"ğŸ”§ ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {device}")

# ìš”ì²­/ì‘ë‹µ ëª¨ë¸
class SearchRequest(BaseModel):
    query: str
    k: int = 5

class SearchResult(BaseModel):
    image_path: str
    caption: str
    score: float
    image_base64: str = ""

# ë¶„ì„ëœ ë°ì´í„° ì €ì¥
analyzed_data = {
    "embeddings": None,
    "image_paths": [],
    "captions": [],
    "images": []
}

@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ì‹œ AI ëª¨ë¸ ë¡œë“œ ë° MLflow ì´ˆê¸°í™”"""
    global yolo_model, clip_model, clip_preprocess
    
    try:
        print("ğŸ¤– AI ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        # MLflow run ì‹œì‘ (ëª¨ë¸ ë¡œë”© ì¶”ì )
        with mlflow.start_run(run_name="Model_Loading") as run:
            mlflow.log_param("device", device)
            mlflow.log_param("startup_time", datetime.now().isoformat())
            
            start_time = time.time()
            
            # YOLO ëª¨ë¸ ë¡œë“œ
            print("ğŸ“¦ YOLO ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘...")
            yolo_model = YOLO('yolov8n.pt')
            yolo_load_time = time.time() - start_time
            mlflow.log_metric("yolo_load_time_seconds", yolo_load_time)
            
            # CLIP ëª¨ë¸ ë¡œë“œ
            print("ğŸ§  CLIP ëª¨ë¸ ë¡œë”© ì¤‘...")
            clip_start = time.time()
            clip_model, clip_preprocess = clip.load("ViT-B/32", device=device)
            clip_load_time = time.time() - clip_start
            mlflow.log_metric("clip_load_time_seconds", clip_load_time)
            
            total_load_time = time.time() - start_time
            mlflow.log_metric("total_model_load_time_seconds", total_load_time)
            
            print("âœ… ëª¨ë“  AI ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
            print(f"   - YOLO: ì‚¬ëŒ íƒì§€ ì¤€ë¹„ì™„ë£Œ ({yolo_load_time:.2f}ì´ˆ)")
            print(f"   - CLIP: ìì—°ì–´ ê²€ìƒ‰ ì¤€ë¹„ì™„ë£Œ ({clip_load_time:.2f}ì´ˆ)")
            print(f"   - ì´ ë¡œë”© ì‹œê°„: {total_load_time:.2f}ì´ˆ")
            
    except Exception as e:
        print(f"âŒ AI ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        if mlflow.active_run():
            mlflow.log_param("loading_error", str(e))

@app.get("/")
async def ë©”ì¸í˜ì´ì§€():
    """ì„œë²„ ë©”ì¸ í˜ì´ì§€"""
    return {
        "ë©”ì‹œì§€": "ğŸ¤– MLflow í†µí•© AI ë¶„ì„ ì„œë²„ ì‹¤í–‰ì¤‘!",
        "YOLO_ëª¨ë¸": "ë¡œë”©ë¨" if yolo_model else "ë¡œë”©ì‹¤íŒ¨",
        "CLIP_ëª¨ë¸": "ë¡œë”©ë¨" if clip_model else "ë¡œë”©ì‹¤íŒ¨",
        "ë””ë°”ì´ìŠ¤": device,
        "ë¶„ì„ëœ_ì´ë¯¸ì§€": len(analyzed_data["image_paths"]),
        "MLflow_ì¶”ì ": "í™œì„±í™”ë¨"
    }

@app.get("/health")
async def ìƒíƒœí™•ì¸():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    return {
        "ìƒíƒœ": "ì •ìƒ",
        "AI_ì¤€ë¹„": all([yolo_model, clip_model]),
        "ë””ë°”ì´ìŠ¤": device,
        "ë¶„ì„ëœ_ë°ì´í„°": len(analyzed_data["image_paths"]),
        "MLflow_ì„œë²„": "http://localhost:5000"
    }

@app.get("/stats/performance")
async def ì„±ëŠ¥í†µê³„():
    """MLflow ê¸°ë°˜ ì„±ëŠ¥ í†µê³„"""
    return {
        "ì „ì²´_ì„±ëŠ¥": {
            "total_videos_processed": performance_stats["total_videos_processed"],
            "total_persons_detected": performance_stats["total_persons_detected"],
            "total_searches_performed": performance_stats["total_searches_performed"],
            "average_detection_confidence": performance_stats["average_detection_confidence"],
            "average_processing_time": np.mean(performance_stats["processing_times"]) if performance_stats["processing_times"] else 0
        },
        "ì‹œìŠ¤í…œ_ìƒíƒœ": {
            "YOLO_ë¡œë”©ë¨": yolo_model is not None,
            "CLIP_ë¡œë”©ë¨": clip_model is not None,
            "ë””ë°”ì´ìŠ¤": device,
            "GPU_ì‚¬ìš©ê°€ëŠ¥": torch.cuda.is_available()
        },
        "í˜„ì¬_MLflow_ì‹¤í–‰": current_mlflow_info
    }

def detect_persons_in_video(video_path: Path, max_frames: int = 30, run_id: str = None):
    """ë¹„ë””ì˜¤ì—ì„œ ì‚¬ëŒ íƒì§€ ë° crop ì´ë¯¸ì§€ ìƒì„± (MLflow ì¶”ì  í¬í•¨)"""
    
    if not yolo_model:
        raise Exception("YOLO ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    print(f"ğŸ¬ ë¹„ë””ì˜¤ ë¶„ì„ ì‹œì‘: {video_path.name}")
    
    # MLflow ë©”íŠ¸ë¦­ ë¡œê¹…ì„ ìœ„í•œ ì„¤ì •
    if run_id:
        mlflow.log_param("video_filename", video_path.name)
        mlflow.log_param("max_frames_to_analyze", max_frames)
    
    cap = cv2.VideoCapture(str(video_path))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    
    detected_persons = []
    frame_count = 0
    confidences = []
    
    # í”„ë ˆì„ ê°„ê²© ê³„ì‚°
    frame_interval = max(1, total_frames // max_frames)
    
    print(f"ğŸ“Š ì´ í”„ë ˆì„: {total_frames}, ë¶„ì„í•  í”„ë ˆì„: {min(max_frames, total_frames)}")
    
    analysis_start_time = time.time()
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
            
        frame_count += 1
        
        if frame_count % frame_interval != 0:
            continue
            
        try:
            # YOLOë¡œ ì‚¬ëŒ íƒì§€
            results = yolo_model(frame, classes=[0], verbose=False)
            
            for r in results:
                boxes = r.boxes
                if boxes is not None:
                    for box in boxes:
                        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                        confidence = box.conf[0].cpu().numpy()
                        
                        if confidence > 0.5:
                            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)
                            
                            if (x2 - x1) > 50 and (y2 - y1) > 50:
                                person_img = frame[y1:y2, x1:x2]
                                person_pil = Image.fromarray(cv2.cvtColor(person_img, cv2.COLOR_BGR2RGB))
                                
                                filename = f"{video_path.stem}_frame{frame_count}_person{len(detected_persons)}.jpg"
                                crop_path = CROP_DIR / filename
                                person_pil.save(crop_path)
                                
                                detected_persons.append({
                                    "íŒŒì¼ê²½ë¡œ": str(crop_path),
                                    "í”„ë ˆì„ë²ˆí˜¸": frame_count,
                                    "ì‹ ë¢°ë„": float(confidence),
                                    "ë°•ìŠ¤ì¢Œí‘œ": [x1, y1, x2, y2],
                                    "ì´ë¯¸ì§€": person_pil
                                })
                                
                                confidences.append(float(confidence))
                                print(f"âœ… ì‚¬ëŒ íƒì§€: í”„ë ˆì„ {frame_count}, ì‹ ë¢°ë„ {confidence:.2f}")
        
        except Exception as e:
            print(f"âš ï¸ í”„ë ˆì„ {frame_count} ë¶„ì„ ì‹¤íŒ¨: {e}")
            continue
    
    cap.release()
    analysis_time = time.time() - analysis_start_time
    
    # MLflowì— ë¶„ì„ ê²°ê³¼ ê¸°ë¡
    if run_id:
        mlflow.log_metric("total_frames", total_frames)
        mlflow.log_metric("analyzed_frames", frame_count)
        mlflow.log_metric("persons_detected", len(detected_persons))
        mlflow.log_metric("analysis_time_seconds", analysis_time)
        mlflow.log_metric("fps", fps)
        
        if confidences:
            mlflow.log_metric("average_detection_confidence", np.mean(confidences))
            mlflow.log_metric("max_detection_confidence", np.max(confidences))
            mlflow.log_metric("min_detection_confidence", np.min(confidences))
        
        # ì›ë³¸ ë¹„ë””ì˜¤ë¥¼ artifactë¡œ ì €ì¥
        mlflow.log_artifact(str(video_path), "input_videos")
    
    print(f"ğŸ‰ ë¶„ì„ ì™„ë£Œ: {len(detected_persons)}ëª…ì˜ ì‚¬ëŒ íƒì§€ ({analysis_time:.2f}ì´ˆ)")
    
    return detected_persons

def generate_clip_embeddings(detected_persons, run_id: str = None):
    """íƒì§€ëœ ì‚¬ëŒë“¤ì˜ CLIP ì„ë² ë”© ìƒì„± (MLflow ì¶”ì  í¬í•¨)"""
    
    if not clip_model:
        raise Exception("CLIP ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    print("ğŸ§  CLIP ì„ë² ë”© ìƒì„± ì¤‘...")
    
    embedding_start_time = time.time()
    
    embeddings = []
    image_paths = []
    captions = []
    images = []
    
    for i, person in enumerate(detected_persons):
        try:
            image = person["ì´ë¯¸ì§€"]
            image_input = clip_preprocess(image).unsqueeze(0).to(device)
            
            with torch.no_grad():
                image_features = clip_model.encode_image(image_input)
                image_features = image_features.cpu().numpy()[0]
            
            embeddings.append(image_features)
            image_paths.append(person["íŒŒì¼ê²½ë¡œ"])
            
            caption = f"í”„ë ˆì„ {person['í”„ë ˆì„ë²ˆí˜¸']}ì—ì„œ íƒì§€ëœ ì¸ë¬¼ (ì‹ ë¢°ë„: {person['ì‹ ë¢°ë„']:.1%})"
            captions.append(caption)
            
            # Base64 ì¸ì½”ë”©
            buffered = io.BytesIO()
            image.save(buffered, format="JPEG")
            img_base64 = base64.b64encode(buffered.getvalue()).decode()
            images.append(img_base64)
            
        except Exception as e:
            print(f"âš ï¸ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨ {i}: {e}")
            continue
    
    embedding_time = time.time() - embedding_start_time
    
    # MLflowì— ì„ë² ë”© ìƒì„± ê²°ê³¼ ê¸°ë¡
    if run_id:
        mlflow.log_metric("embeddings_generated", len(embeddings))
        mlflow.log_metric("embedding_generation_time_seconds", embedding_time)
        mlflow.log_param("clip_model", "ViT-B/32")
        mlflow.log_param("embedding_dimension", len(embeddings[0]) if embeddings else 0)
    
    print(f"âœ… {len(embeddings)}ê°œì˜ ì„ë² ë”© ìƒì„± ì™„ë£Œ ({embedding_time:.2f}ì´ˆ)")
    
    return {
        "embeddings": np.vstack(embeddings) if embeddings else None,
        "image_paths": image_paths,
        "captions": captions,
        "images": images
    }

@app.post("/upload-video")
async def ì˜ìƒì—…ë¡œë“œ(file: UploadFile = File(...)):
    """CCTV ì˜ìƒ ì—…ë¡œë“œ ë° ì‹¤ì œ AI ë¶„ì„ (MLflow ì¶”ì  í¬í•¨)"""
    
    print(f"ğŸ“¹ ì—…ë¡œë“œ ì‹œì‘: {file.filename}")
    
    if not file.filename.lower().endswith(('.mp4', '.avi', '.mov', '.mkv')):
        raise HTTPException(status_code=400, detail="ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤")
    
    # MLflow run ì‹œì‘
    with mlflow.start_run(run_name=f"Video_Analysis_{file.filename}") as run:
        global current_mlflow_info, performance_stats
        
        current_mlflow_info["run_id"] = run.info.run_id
        current_mlflow_info["experiment_id"] = run.info.experiment_id
        
        try:
            # íŒŒì¼ ì €ì¥
            video_path = VIDEO_DIR / file.filename
            with open(video_path, "wb") as buffer:
                shutil.copyfileobj(file.file, buffer)
            
            # MLflowì— ê¸°ë³¸ ì •ë³´ ê¸°ë¡
            mlflow.log_param("filename", file.filename)
            mlflow.log_param("file_size_mb", video_path.stat().st_size / (1024*1024))
            mlflow.log_param("upload_timestamp", datetime.now().isoformat())
            
            print(f"ğŸ’¾ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {video_path}")
            
            if not yolo_model or not clip_model:
                mlflow.log_param("status", "models_not_loaded")
                return {
                    "status": "warning",
                    "message": "AI ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•„ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                    "mlflow_run_id": run.info.run_id,
                    "mlflow_experiment_id": run.info.experiment_id
                }
            
            # 1. YOLOë¡œ ì‚¬ëŒ íƒì§€
            detected_persons = detect_persons_in_video(video_path, max_frames=20, run_id=run.info.run_id)
            
            if not detected_persons:
                mlflow.log_param("status", "no_persons_detected")
                return {
                    "status": "success",
                    "message": "ì˜ìƒ ë¶„ì„ ì™„ë£Œ, í•˜ì§€ë§Œ íƒì§€ëœ ì‚¬ëŒì´ ì—†ìŠµë‹ˆë‹¤",
                    "total_crops": 0,
                    "mlflow_run_id": run.info.run_id,
                    "mlflow_experiment_id": run.info.experiment_id
                }
            
            # 2. CLIP ì„ë² ë”© ìƒì„±
            embedding_data = generate_clip_embeddings(detected_persons, run_id=run.info.run_id)
            
            # 3. ê¸€ë¡œë²Œ ë°ì´í„°ì— ì €ì¥
            global analyzed_data
            analyzed_data = embedding_data
            
            # 4. ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸
            performance_stats["total_videos_processed"] += 1
            performance_stats["total_persons_detected"] += len(detected_persons)
            avg_confidence = np.mean([p["ì‹ ë¢°ë„"] for p in detected_persons])
            performance_stats["average_detection_confidence"] = avg_confidence
            
            # MLflowì— ìµœì¢… ìƒíƒœ ê¸°ë¡
            mlflow.log_param("status", "success")
            mlflow.log_metric("final_persons_count", len(detected_persons))
            mlflow.log_metric("final_embeddings_count", len(embedding_data["embeddings"]) if embedding_data["embeddings"] is not None else 0)
            
            return {
                "status": "success", 
                "message": f"'{file.filename}' ë¶„ì„ ì™„ë£Œ!",
                "total_crops": len(detected_persons),
                "ë¶„ì„ê²°ê³¼": f"{len(detected_persons)}ëª…ì˜ ì‹¤ì œ ì¸ë¬¼ì´ AIë¡œ íƒì§€ë˜ì—ˆìŠµë‹ˆë‹¤",
                "mlflow_run_id": run.info.run_id,
                "mlflow_experiment_id": run.info.experiment_id
            }
            
        except Exception as e:
            mlflow.log_param("status", "error")
            mlflow.log_param("error_message", str(e))
            print(f"âŒ ë¶„ì„ ì˜¤ë¥˜: {e}")
            raise HTTPException(status_code=500, detail=f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")

@app.post("/search", response_model=List[SearchResult])
async def ì¸ë¬¼ê²€ìƒ‰(request: SearchRequest):
    """ì‹¤ì œ CLIP ê¸°ë°˜ ìì—°ì–´ ê²€ìƒ‰ (MLflow ì¶”ì  í¬í•¨)"""
    
    print(f"ğŸ” ì‹¤ì œ AI ê²€ìƒ‰: '{request.query}'")
    
    if analyzed_data["embeddings"] is None:
        raise HTTPException(status_code=404, detail="ë¨¼ì € ì˜ìƒì„ ì—…ë¡œë“œí•˜ê³  ë¶„ì„í•´ì£¼ì„¸ìš”")
    
    if not clip_model:
        raise HTTPException(status_code=500, detail="CLIP ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    # MLflow run ì‹œì‘ (ê²€ìƒ‰ ì¶”ì )
    with mlflow.start_run(run_name=f"Search_{request.query[:20]}", nested=True) as search_run:
        global performance_stats
        
        try:
            search_start_time = time.time()
            
            # MLflowì— ê²€ìƒ‰ íŒŒë¼ë¯¸í„° ê¸°ë¡
            mlflow.log_param("search_query", request.query)
            mlflow.log_param("k", request.k)
            mlflow.log_param("search_timestamp", datetime.now().isoformat())
            
            # ê²€ìƒ‰ì–´ë¥¼ CLIPìœ¼ë¡œ ì¸ì½”ë”©
            text_input = clip.tokenize([request.query]).to(device)
            
            with torch.no_grad():
                text_features = clip_model.encode_text(text_input)
                text_features = text_features.cpu().numpy()[0]
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            image_embeddings = analyzed_data["embeddings"]
            similarities = np.dot(image_embeddings, text_features) / (
                np.linalg.norm(image_embeddings, axis=1) * np.linalg.norm(text_features)
            )
            
            # ìƒìœ„ kê°œ ê²°ê³¼ ì„ íƒ
            top_indices = np.argsort(-similarities)[:request.k]
            
            results = []
            similarity_scores = []
            
            for idx in top_indices:
                if similarities[idx] > 0.1:  # ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’
                    results.append(SearchResult(
                        image_path=analyzed_data["image_paths"][idx],
                        caption=analyzed_data["captions"][idx],
                        score=float(similarities[idx]),
                        image_base64=analyzed_data["images"][idx]
                    ))
                    similarity_scores.append(float(similarities[idx]))
            
            search_time = time.time() - search_start_time
            
            # MLflowì— ê²€ìƒ‰ ê²°ê³¼ ê¸°ë¡
            mlflow.log_metric("search_results_count", len(results))
            mlflow.log_metric("search_time_seconds", search_time)
            mlflow.log_metric("max_similarity_score", np.max(similarity_scores) if similarity_scores else 0)
            mlflow.log_metric("avg_similarity_score", np.mean(similarity_scores) if similarity_scores else 0)
            mlflow.log_metric("min_similarity_threshold", 0.1)
            
            # ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸
            performance_stats["total_searches_performed"] += 1
            performance_stats["processing_times"].append(search_time)
            
            print(f"âœ… ì‹¤ì œ AI ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼ ({search_time:.2f}ì´ˆ)")
            return results
            
        except Exception as e:
            mlflow.log_param("search_error", str(e))
            print(f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            raise HTTPException(status_code=500, detail=f"ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")

@app.get("/image/{filename}")
async def get_image(filename: str):
    """crop ì´ë¯¸ì§€ íŒŒì¼ ë°˜í™˜"""
    image_path = CROP_DIR / filename
    if image_path.exists():
        return FileResponse(image_path)
    else:
        raise HTTPException(status_code=404, detail="ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

@app.get("/stats")
async def í†µê³„():
    """ì„œë²„ í†µê³„"""
    return {
        "AI_ëª¨ë¸_ìƒíƒœ": {
            "YOLO": "ë¡œë”©ë¨" if yolo_model else "ë¡œë”©ì‹¤íŒ¨",
            "CLIP": "ë¡œë”©ë¨" if clip_model else "ë¡œë”©ì‹¤íŒ¨"
        },
        "ë¶„ì„_í†µê³„": {
            "íƒì§€ëœ_ì¸ë¬¼": len(analyzed_data["image_paths"]),
            "ì„ë² ë”©_ìƒì„±ë¨": analyzed_data["embeddings"] is not None
        },
        "ì‹œìŠ¤í…œ_ì •ë³´": {
            "ë””ë°”ì´ìŠ¤": device,
            "GPU_ì‚¬ìš©ê°€ëŠ¥": torch.cuda.is_available()
        },
        "MLflow_ì •ë³´": current_mlflow_info
    }

# ì„œë²„ ì‹¤í–‰
if __name__ == "__main__":
    print("ğŸš€ MLflow í†µí•© AI ë¶„ì„ ì„œë²„ ì‹œì‘!")
    print("ğŸ¤– YOLO + CLIP + MLflow ê¸°ë°˜ ì¸ë¬¼ íƒì§€ ë° ê²€ìƒ‰")
    print("ğŸ“ AI ì„œë²„: http://localhost:8001")
    print("ğŸ“ MLflow UI: http://localhost:5000")
    print("ğŸ“š API ë¬¸ì„œ: http://localhost:8001/docs")
    print("=" * 60)
    
    uvicorn.run(app, host="0.0.0.0", port=8001)