# main.py - Render ë°°í¬ìš© ìµœì í™” ë²„ì „
import os
from fastapi import FastAPI, UploadFile, File, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import torch
import numpy as np
from PIL import Image
import cv2
import io
import base64
import tempfile
from typing import List
import uvicorn
import asyncio
from pathlib import Path

# í™˜ê²½ ë³€ìˆ˜
PORT = int(os.getenv("PORT", 8001))
ENVIRONMENT = os.getenv("ENVIRONMENT", "development")

app = FastAPI(
    title="CCTV AI Analysis Server - Render Optimized",
    version="1.0.0",
    description="Lightweight AI server optimized for Render deployment"
)

# CORS ì„¤ì • (Renderìš©)
if ENVIRONMENT == "production":
    # í”„ë¡œë•ì…˜: íŠ¹ì • ë„ë©”ì¸ë§Œ í—ˆìš©
    allowed_origins = [
        "https://your-frontend-app.onrender.com",
        "https://localhost:3000"  # ë¡œì»¬ ê°œë°œìš©
    ]
else:
    # ê°œë°œ: ëª¨ë“  ë„ë©”ì¸ í—ˆìš©
    allowed_origins = ["*"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=allowed_origins,
    allow_credentials=True,
    allow_methods=["GET", "POST"],
    allow_headers=["*"],
)

# ì „ì—­ ë³€ìˆ˜
device = 'cpu'  # Renderì—ì„œëŠ” CPUë§Œ ì‚¬ìš©
models_loaded = False
yolo_model = None
clip_model = None
clip_preprocess = None

# ìš”ì²­/ì‘ë‹µ ëª¨ë¸
class SearchRequest(BaseModel):
    query: str
    k: int = 5

class SearchResult(BaseModel):
    image_path: str
    caption: str
    score: float
    image_base64: str = ""

# ë©”ëª¨ë¦¬ ë‚´ ì„ì‹œ ì €ì¥ (Render ì¬ì‹œì‘ ëŒ€ì‘)
temp_storage = {
    "embeddings": None,
    "captions": [],
    "images": []
}

async def load_models():
    """ëª¨ë¸ ì§€ì—° ë¡œë”© (ë©”ëª¨ë¦¬ ìµœì í™”)"""
    global yolo_model, clip_model, clip_preprocess, models_loaded
    
    if models_loaded:
        return
    
    try:
        print("ğŸ¤– ê²½ëŸ‰ AI ëª¨ë¸ ë¡œë”© ì‹œì‘...")
        
        # YOLO ê²½ëŸ‰ ëª¨ë¸ (nano ë²„ì „)
        from ultralytics import YOLO
        yolo_model = YOLO('yolov8n.pt')  # ê°€ì¥ ì‘ì€ ëª¨ë¸
        print("âœ… YOLO nano ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        
        # CLIP ê²½ëŸ‰ ëª¨ë¸
        try:
            import clip
            clip_model, clip_preprocess = clip.load("ViT-B/32", device=device)
            print("âœ… CLIP ViT-B/32 ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ CLIP ë¡œë”© ì‹¤íŒ¨: {e}")
            # CLIP ëŒ€ì‹  ê°„ë‹¨í•œ íŠ¹ì§• ì¶”ì¶œê¸° ì‚¬ìš©
            clip_model = None
            
        models_loaded = True
        print("ğŸ‰ ëª¨ë“  ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        models_loaded = False

@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ì´ˆê¸°í™”"""
    print("ğŸš€ Render ìµœì í™” AI ì„œë²„ ì‹œì‘")
    print(f"ğŸŒ í™˜ê²½: {ENVIRONMENT}")
    print(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {device}")
    print(f"ğŸ“¦ Python: {os.sys.version}")
    
    # ëª¨ë¸ì€ ì²« ìš”ì²­ ì‹œ ë¡œë”© (ì‹œì‘ ì‹œê°„ ë‹¨ì¶•)
    print("â³ ëª¨ë¸ì€ ì²« ìš”ì²­ ì‹œ ë¡œë”©ë©ë‹ˆë‹¤...")

@app.get("/")
async def root():
    """í—¬ìŠ¤ì²´í¬ ë° ìƒíƒœ í™•ì¸"""
    return {
        "status": "healthy",
        "service": "CCTV AI Analysis Server",
        "environment": ENVIRONMENT,
        "models_loaded": models_loaded,
        "device": device,
        "message": "âœ… Render ë°°í¬ ì„±ê³µ!"
    }

@app.get("/health")
async def health_check():
    """ìƒì„¸ í—¬ìŠ¤ì²´í¬"""
    return {
        "status": "healthy",
        "models": {
            "yolo": yolo_model is not None,
            "clip": clip_model is not None
        },
        "memory_usage": "optimized_for_render",
        "temp_data": len(temp_storage["images"])
    }

async def process_video_lightweight(video_bytes: bytes) -> List[dict]:
    """ê²½ëŸ‰ ë¹„ë””ì˜¤ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ìµœì í™”)"""
    
    if not models_loaded:
        await load_models()
    
    if not yolo_model:
        raise HTTPException(status_code=500, detail="YOLO ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    detected_persons = []
    
    try:
        # ì„ì‹œ íŒŒì¼ë¡œ ë¹„ë””ì˜¤ ì €ì¥
        with tempfile.NamedTemporaryFile(suffix='.mp4', delete=False) as tmp_file:
            tmp_file.write(video_bytes)
            tmp_path = tmp_file.name
        
        # OpenCVë¡œ ë¹„ë””ì˜¤ ì½ê¸°
        cap = cv2.VideoCapture(tmp_path)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = int(cap.get(cv2.CAP_PROP_FPS)) or 30
        
        # ì²˜ë¦¬í•  í”„ë ˆì„ ìˆ˜ ì œí•œ (Render íƒ€ì„ì•„ì›ƒ ë°©ì§€)
        max_frames = min(10, total_frames // 5)  # ìµœëŒ€ 10í”„ë ˆì„ë§Œ
        frame_interval = max(1, total_frames // max_frames)
        
        frame_count = 0
        processed_frames = 0
        
        print(f"ğŸ“¹ ë¹„ë””ì˜¤ ì²˜ë¦¬: {total_frames}í”„ë ˆì„ ì¤‘ {max_frames}í”„ë ˆì„ ë¶„ì„")
        
        while processed_frames < max_frames:
            ret, frame = cap.read()
            if not ret:
                break
                
            frame_count += 1
            
            # ì§€ì •ëœ ê°„ê²©ìœ¼ë¡œë§Œ ì²˜ë¦¬
            if frame_count % frame_interval != 0:
                continue
            
            try:
                # YOLO ì¶”ë¡  (ë¹ ë¥¸ ì²˜ë¦¬)
                results = yolo_model(frame, classes=[0], verbose=False, imgsz=416)  # ì‘ì€ ì´ë¯¸ì§€ë¡œ ì²˜ë¦¬
                
                for r in results:
                    boxes = r.boxes
                    if boxes is not None:
                        for box in boxes:
                            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                            confidence = box.conf[0].cpu().numpy()
                            
                            # ë†’ì€ ì‹ ë¢°ë„ë§Œ ì„ íƒ (ì†ë„ í–¥ìƒ)
                            if confidence > 0.7:
                                x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)
                                
                                # ìµœì†Œ í¬ê¸° í™•ì¸
                                if (x2 - x1) > 40 and (y2 - y1) > 40:
                                    # ì‚¬ëŒ ì˜ì—­ crop
                                    person_img = frame[y1:y2, x1:x2]
                                    person_pil = Image.fromarray(cv2.cvtColor(person_img, cv2.COLOR_BGR2RGB))
                                    
                                    # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì • (ë©”ëª¨ë¦¬ ì ˆì•½)
                                    if person_pil.size[0] > 200 or person_pil.size[1] > 200:
                                        person_pil.thumbnail((200, 200), Image.Resampling.LANCZOS)
                                    
                                    detected_persons.append({
                                        "frame": frame_count,
                                        "confidence": float(confidence),
                                        "image": person_pil,
                                        "bbox": [x1, y1, x2, y2]
                                    })
                                    
                                    print(f"âœ… ì‚¬ëŒ íƒì§€: í”„ë ˆì„ {frame_count}, ì‹ ë¢°ë„ {confidence:.2f}")
                                    
                                    # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ìµœëŒ€ 5ëª…ê¹Œì§€ë§Œ
                                    if len(detected_persons) >= 5:
                                        break
                        
                        if len(detected_persons) >= 5:
                            break
                            
            except Exception as e:
                print(f"âš ï¸ í”„ë ˆì„ {frame_count} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
            
            processed_frames += 1
        
        cap.release()
        
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        try:
            os.unlink(tmp_path)
        except:
            pass
            
        print(f"ğŸ‰ ì²˜ë¦¬ ì™„ë£Œ: {len(detected_persons)}ëª… íƒì§€")
        return detected_persons
        
    except Exception as e:
        print(f"âŒ ë¹„ë””ì˜¤ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ë¹„ë””ì˜¤ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")

async def generate_embeddings_lightweight(persons: List[dict]) -> dict:
    """ê²½ëŸ‰ ì„ë² ë”© ìƒì„±"""
    
    if not clip_model:
        # CLIPì´ ì—†ëŠ” ê²½ìš° ê°„ë‹¨í•œ íŠ¹ì§• ì‚¬ìš©
        print("âš ï¸ CLIP ëª¨ë¸ ì—†ìŒ, ê°„ë‹¨í•œ íŠ¹ì§• ì‚¬ìš©")
        embeddings = []
        for i, person in enumerate(persons):
            # ê°„ë‹¨í•œ íŠ¹ì§• ë²¡í„° ìƒì„± (ì´ë¯¸ì§€ íˆìŠ¤í† ê·¸ë¨ ê¸°ë°˜)
            img_array = np.array(person["image"])
            hist = cv2.calcHist([img_array], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])
            feature = hist.flatten()
            feature = feature / np.linalg.norm(feature)  # ì •ê·œí™”
            embeddings.append(feature)
    else:
        # CLIP ì‚¬ìš©
        embeddings = []
        for person in persons:
            try:
                image_input = clip_preprocess(person["image"]).unsqueeze(0).to(device)
                with torch.no_grad():
                    image_features = clip_model.encode_image(image_input)
                    embeddings.append(image_features.cpu().numpy()[0])
            except:
                # ì‹¤íŒ¨ ì‹œ ì œë¡œ ë²¡í„°
                embeddings.append(np.zeros(512))
    
    # Base64 ì´ë¯¸ì§€ ìƒì„±
    images_b64 = []
    captions = []
    
    for i, person in enumerate(persons):
        # ìº¡ì…˜ ìƒì„±
        caption = f"í”„ë ˆì„ {person['frame']}ì—ì„œ íƒì§€ëœ ì¸ë¬¼ (ì‹ ë¢°ë„: {person['confidence']:.1%})"
        captions.append(caption)
        
        # Base64 ì¸ì½”ë”©
        buffered = io.BytesIO()
        person["image"].save(buffered, format="JPEG", optimize=True, quality=85)
        img_b64 = base64.b64encode(buffered.getvalue()).decode()
        images_b64.append(img_b64)
    
    return {
        "embeddings": np.vstack(embeddings) if embeddings else None,
        "captions": captions,
        "images": images_b64
    }

@app.post("/upload-video")
async def upload_video(background_tasks: BackgroundTasks, file: UploadFile = File(...)):
    """ë¹„ë””ì˜¤ ì—…ë¡œë“œ ë° ë¶„ì„ (Render ìµœì í™”)"""
    
    print(f"ğŸ“¹ íŒŒì¼ ì—…ë¡œë“œ: {file.filename}")
    
    # íŒŒì¼ í¬ê¸° ì œí•œ (Render ë©”ëª¨ë¦¬ í•œê³„)
    max_size = 50 * 1024 * 1024  # 50MB
    content = await file.read()
    
    if len(content) > max_size:
        raise HTTPException(status_code=413, detail="íŒŒì¼ì´ ë„ˆë¬´ í½ë‹ˆë‹¤ (ìµœëŒ€ 50MB)")
    
    if not file.filename.lower().endswith(('.mp4', '.avi', '.mov')):
        raise HTTPException(status_code=400, detail="ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤")
    
    try:
        # ë¹„ë””ì˜¤ ì²˜ë¦¬
        detected_persons = await process_video_lightweight(content)
        
        if not detected_persons:
            return {
                "status": "success",
                "message": "ë¶„ì„ ì™„ë£Œ, íƒì§€ëœ ì‚¬ëŒ ì—†ìŒ",
                "total_crops": 0
            }
        
        # ì„ë² ë”© ìƒì„±
        embedding_data = await generate_embeddings_lightweight(detected_persons)
        
        # ë©”ëª¨ë¦¬ì— ì €ì¥ (RenderëŠ” íŒŒì¼ ì‹œìŠ¤í…œì´ ì„ì‹œì )
        global temp_storage
        temp_storage = embedding_data
        
        return {
            "status": "success",
            "message": f"'{file.filename}' ë¶„ì„ ì™„ë£Œ!",
            "total_crops": len(detected_persons),
            "note": "âœ… Render ë°°í¬ ì„±ê³µ - ê²½ëŸ‰ ì²˜ë¦¬ ì™„ë£Œ"
        }
        
    except Exception as e:
        print(f"âŒ ì—…ë¡œë“œ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")

@app.post("/search", response_model=List[SearchResult])
async def search_persons(request: SearchRequest):
    """ê²½ëŸ‰ ê²€ìƒ‰ ê¸°ëŠ¥"""
    
    if temp_storage["embeddings"] is None:
        raise HTTPException(status_code=404, detail="ë¨¼ì € ì˜ìƒì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”")
    
    try:
        if clip_model:
            # CLIP ì‚¬ìš© ê²€ìƒ‰
            import clip
            text_input = clip.tokenize([request.query]).to(device)
            
            with torch.no_grad():
                text_features = clip_model.encode_text(text_input).cpu().numpy()[0]
            
            # ìœ ì‚¬ë„ ê³„ì‚°
            similarities = np.dot(temp_storage["embeddings"], text_features) / (
                np.linalg.norm(temp_storage["embeddings"], axis=1) * np.linalg.norm(text_features)
            )
        else:
            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­
            similarities = np.random.random(len(temp_storage["captions"]))  # ë°ëª¨ìš©
        
        # ìƒìœ„ ê²°ê³¼ ì„ íƒ
        top_indices = np.argsort(-similarities)[:request.k]
        
        results = []
        for idx in top_indices:
            if similarities[idx] > 0.1:
                results.append(SearchResult(
                    image_path=f"temp_image_{idx}",
                    caption=temp_storage["captions"][idx],
                    score=float(similarities[idx]),
                    image_base64=temp_storage["images"][idx]
                ))
        
        return results
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")

@app.get("/clear-cache")
async def clear_cache():
    """ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬ (Render ë©”ëª¨ë¦¬ ê´€ë¦¬)"""
    global temp_storage
    temp_storage = {"embeddings": None, "captions": [], "images": []}
    return {"status": "success", "message": "ìºì‹œê°€ ì •ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤"}

# Render ë°°í¬ìš© ì‹¤í–‰
if __name__ == "__main__":
    uvicorn.run(
        app, 
        host="0.0.0.0", 
        port=PORT,
        log_level="info"
    )